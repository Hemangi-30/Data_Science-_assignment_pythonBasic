{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1 :  Define the z-statistic and explain its relationship to the standard normal distribution. How is the z-statistic used in hypothesis testing?\n",
        "\n",
        "\n",
        "The z-statistic is a measure that quantifies the distance, in standard deviations, a data point or sample mean is from the population mean. It is calculated using the formula:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "(\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        ")/\n",
        "ùúé\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "ùëã\n",
        "X is the value of the data point or sample mean,\n",
        "\n",
        "ùúá\n",
        "Œº is the population mean,\n",
        "\n",
        "ùúé\n",
        "œÉ is the standard deviation of the population.\n",
        "\n",
        "Relationship to the Standard Normal Distribution\n",
        "The z-statistic follows a standard normal distribution, which is a normal distribution with a mean of 0 and a standard deviation of 1. This allows for easy interpretation and comparison of z-scores across different datasets. Because the standard normal distribution is well-studied, we can use z-scores to determine probabilities and critical values when conducting statistical tests.\n",
        "\n",
        "Use of the z-statistic in Hypothesis Testing\n",
        "In hypothesis testing, the z-statistic is used to determine how far a sample mean deviates from the hypothesized population mean under the null hypothesis. Here‚Äôs a brief overview of the process:\n",
        "\n",
        "State the Hypotheses:\n",
        "\n",
        "Null Hypothesis (\n",
        "ùêª\n",
        "0\n",
        "H\n",
        "0\n",
        "‚Äã\n",
        " ): Assumes no effect or no difference.\n",
        "Alternative Hypothesis (\n",
        "ùêª\n",
        "ùëé\n",
        "H\n",
        "a\n",
        "‚Äã\n",
        " ): Assumes there is an effect or a difference.\n",
        "Collect Data: Obtain a sample from the population.\n",
        "\n",
        "Calculate the z-statistic: Use the formula mentioned above. For a sample mean, the formula modifies to:\n",
        "\n",
        "ùëß\n",
        "=\n",
        "(\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        ")/\n",
        "(\n",
        "ùúé\n",
        "/\n",
        "square root(ùëõ)\n",
        ")\n",
        "\n",
        "\n",
        "where\n",
        "ùëã\n",
        "Àâ\n",
        "X\n",
        "Àâ\n",
        "  is the sample mean and\n",
        "ùëõ\n",
        "n is the sample size.\n",
        "\n",
        "Determine the Significance Level (\n",
        "ùõº\n",
        "Œ±): Common values are 0.05 or 0.01.\n",
        "\n",
        "Find the Critical z-value: Based on the significance level and the type of test (one-tailed or two-tailed), determine the critical value(s) from the standard normal distribution.\n",
        "\n",
        "Make a Decision:\n",
        "\n",
        "If the calculated z-statistic falls in the critical region (beyond the critical value), reject the null hypothesis.\n",
        "If it does not, fail to reject the null hypothesis.\n",
        "By using the z-statistic, researchers can assess the likelihood of observing the data under the null hypothesis, allowing for informed decisions in the context of statistical inference."
      ],
      "metadata": {
        "id": "whzf1JGrNxGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 :  What is a p-value, and how is it used in hypothesis testing? What does it mean if the p-value is very small (e.g., 0.01)?\n",
        "\n",
        "\n",
        "A **p-value** (probability value) is a statistical measure that helps determine the significance of your test results in hypothesis testing. Specifically, it quantifies the probability of obtaining a test statistic at least as extreme as the one observed, assuming that the null hypothesis (\\( H_0 \\)) is true.\n",
        "\n",
        "### How p-value is Used in Hypothesis Testing\n",
        "\n",
        "1. **State the Hypotheses**:\n",
        "   - Null Hypothesis (\\( H_0 \\)): Assumes no effect or no difference.\n",
        "   - Alternative Hypothesis (\\( H_a \\)): Assumes there is an effect or a difference.\n",
        "\n",
        "2. **Calculate the Test Statistic**: Based on the sample data.\n",
        "\n",
        "3. **Determine the p-value**: This is calculated based on the observed test statistic. It indicates the probability of observing such data if the null hypothesis is true.\n",
        "\n",
        "4. **Compare p-value to Significance Level (\\( \\alpha \\))**:\n",
        "   - If the p-value is less than or equal to \\( \\alpha \\) (commonly set at 0.05), you reject the null hypothesis, indicating that the results are statistically significant.\n",
        "   - If the p-value is greater than \\( \\alpha \\), you fail to reject the null hypothesis, suggesting insufficient evidence to support the alternative hypothesis.\n",
        "\n",
        "### Interpretation of a Very Small p-value (e.g., 0.01)\n",
        "\n",
        "A very small p-value, such as 0.01, indicates that there is only a 1% probability of observing the test results (or more extreme results) if the null hypothesis is true. This has several implications:\n",
        "\n",
        "1. **Strong Evidence Against \\( H_0 \\)**: A p-value of 0.01 suggests strong evidence that the null hypothesis may not be true, leading researchers to reject \\( H_0 \\).\n",
        "\n",
        "2. **Statistical Significance**: If your significance level \\( \\alpha \\) is set at 0.05, a p-value of 0.01 is considered statistically significant, indicating that the observed effect or difference is unlikely to have occurred by random chance.\n",
        "\n",
        "3. **Caution in Interpretation**: While a small p-value indicates strong evidence against the null hypothesis, it does not measure the size of the effect or its practical significance. It simply indicates the strength of evidence against \\( H_0 \\).\n",
        "\n",
        "4. **Context Matters**: The interpretation of a p-value also depends on the context of the study, the design, and the sample size. A small p-value should prompt further investigation but should be considered alongside confidence intervals and effect sizes for a comprehensive understanding of the results."
      ],
      "metadata": {
        "id": "dGK8O2xXO00q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3 :   Compare and contrast the binomial and Bernoulli distributions.\n",
        "\n",
        "\n",
        "The **Bernoulli distribution** and the **binomial distribution** are closely related, both dealing with binary outcomes, but they differ in their definitions and applications. Here‚Äôs a comparison of the two:\n",
        "\n",
        "### Bernoulli Distribution\n",
        "\n",
        "- **Definition**: The Bernoulli distribution describes a single trial with two possible outcomes: \"success\" (usually coded as 1) and \"failure\" (usually coded as 0).\n",
        "- **Parameters**: It has one parameter \\( p \\), which represents the probability of success. The probability of failure is \\( 1 - p \\).\n",
        "- **Probability Mass Function (PMF)**:\n",
        "  \\[\n",
        "  P(X = x) = p^x (1 - p)^{1 - x}, \\quad \\text{for } x \\in \\{0, 1\\}\n",
        "  \\]\n",
        "- **Mean**: The mean (expected value) is \\( E[X] = p \\).\n",
        "- **Variance**: The variance is \\( \\text{Var}(X) = p(1 - p) \\).\n",
        "- **Applications**: It is used for modeling situations where you have one trial, such as flipping a coin once or determining if a patient responds to a treatment.\n",
        "\n",
        "### Binomial Distribution\n",
        "\n",
        "- **Definition**: The binomial distribution extends the Bernoulli distribution to multiple independent trials (n trials), each with the same probability of success \\( p \\).\n",
        "- **Parameters**: It has two parameters: \\( n \\) (the number of trials) and \\( p \\) (the probability of success on each trial).\n",
        "- **Probability Mass Function (PMF)**:\n",
        "  \\[\n",
        "  P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}, \\quad \\text{for } k = 0, 1, 2, \\ldots, n\n",
        "  \\]\n",
        "  where \\( \\binom{n}{k} \\) is the binomial coefficient.\n",
        "- **Mean**: The mean (expected value) is \\( E[X] = np \\).\n",
        "- **Variance**: The variance is \\( \\text{Var}(X) = np(1 - p) \\).\n",
        "- **Applications**: It is used for modeling the number of successes in a fixed number of independent trials, such as counting the number of heads in 10 coin flips.\n",
        "\n",
        "### Summary of Key Differences\n",
        "\n",
        "| Feature               | Bernoulli Distribution                    | Binomial Distribution                          |\n",
        "|----------------------|-------------------------------------------|-----------------------------------------------|\n",
        "| Number of Trials     | 1                                         | \\( n \\) (multiple trials)                     |\n",
        "| Outcomes              | Two outcomes (success/failure)           | Counts of successes in \\( n \\) trials         |\n",
        "| Parameters           | 1 parameter (\\( p \\))                     | 2 parameters (\\( n \\), \\( p \\))               |\n",
        "| PMF                  | \\( P(X = x) = p^x (1 - p)^{1 - x} \\)    | \\( P(X = k) = \\binom{n}{k} p^k (1 - p)^{n-k} \\) |\n",
        "| Mean                 | \\( E[X] = p \\)                           | \\( E[X] = np \\)                               |\n",
        "| Variance             | \\( \\text{Var}(X) = p(1 - p) \\)          | \\( \\text{Var}(X) = np(1 - p) \\)              |\n",
        "\n",
        "In summary, the Bernoulli distribution models a single binary outcome, while the binomial distribution models the number of successes across multiple trials, both being fundamental in probability and statistics."
      ],
      "metadata": {
        "id": "v67CJxr2PNIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4 : Under what conditions is the binomial distribution used, and how does it relate to the Bernoulli distribution?\n",
        "\n",
        "\n",
        "The **binomial distribution** is used under specific conditions that define its applicability. These conditions are:\n",
        "\n",
        "### Conditions for Using the Binomial Distribution\n",
        "\n",
        "1. **Fixed Number of Trials**: There must be a predetermined number of trials, denoted as \\( n \\).\n",
        "\n",
        "2. **Two Possible Outcomes**: Each trial must result in one of two outcomes, typically termed \"success\" and \"failure\".\n",
        "\n",
        "3. **Constant Probability**: The probability of success (\\( p \\)) must remain constant for each trial. The probability of failure will then be \\( 1 - p \\).\n",
        "\n",
        "4. **Independence**: The trials must be independent, meaning the outcome of one trial does not affect the outcomes of others.\n",
        "\n",
        "### Relationship to the Bernoulli Distribution\n",
        "\n",
        "The binomial distribution is fundamentally an extension of the **Bernoulli distribution**. Here's how they relate:\n",
        "\n",
        "- **Single Trial vs. Multiple Trials**: The Bernoulli distribution describes the outcome of a single trial (success or failure). In contrast, the binomial distribution sums the outcomes of \\( n \\) independent Bernoulli trials.\n",
        "\n",
        "- **Parameters**: The Bernoulli distribution has one parameter \\( p \\), while the binomial distribution has two parameters: \\( n \\) (the number of trials) and \\( p \\) (the probability of success in each trial).\n",
        "\n",
        "- **Connection**: If you consider a binomial distribution with \\( n = 1 \\), it reduces to a Bernoulli distribution. Essentially, the binomial distribution counts the number of successes across multiple Bernoulli trials.\n",
        "\n",
        "### Example\n",
        "\n",
        "For instance, if you flip a coin 10 times (binomial distribution with \\( n = 10 \\) and \\( p = 0.5 \\)), each flip represents a Bernoulli trial (success if it lands heads, failure if tails). The binomial distribution would tell you the probability of getting a certain number of heads (successes) in those 10 flips.\n",
        "\n",
        "In summary, the binomial distribution is used for experiments involving multiple independent trials with a fixed number of outcomes, while the Bernoulli distribution applies to individual trials. The two are closely related, with the binomial distribution essentially representing the aggregate of multiple Bernoulli trials."
      ],
      "metadata": {
        "id": "OEnpFrELPfOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5 :  What are the key properties of the Poisson distribution, and when is it appropriate to use this distribution?\n",
        "\n",
        "### Key Properties of the Poisson Distribution\n",
        "\n",
        "1. **Discrete Distribution**: The Poisson distribution models the number of events occurring in a fixed interval of time or space.\n",
        "\n",
        "2. **Parameter**: It is defined by a single parameter \\( \\lambda \\) (lambda), which represents the average number of events in the interval.\n",
        "\n",
        "3. **Probability Mass Function (PMF)**:\n",
        "   Probability Mass Function (PMF):\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "=\n",
        "ùëò\n",
        ")\n",
        "=\n",
        "ùúÜ\n",
        "ùëò\n",
        "ùëí\n",
        "‚àí\n",
        "ùúÜ\n",
        "ùëò\n",
        "!\n",
        ",\n",
        "ùëò\n",
        "=\n",
        "0\n",
        ",\n",
        "1\n",
        ",\n",
        "2\n",
        ",\n",
        "‚Ä¶\n",
        "P(X=k)=\n",
        "k!\n",
        "Œª\n",
        "k\n",
        " e\n",
        "‚àíŒª\n",
        "\n",
        "‚Äã\n",
        " ,k=0,1,2,‚Ä¶\n",
        "4. **Mean and Variance**: Both the mean and variance of a Poisson distribution are equal to \\( \\lambda \\):\n",
        "   -Mean:\n",
        "ùê∏\n",
        "[\n",
        "ùëã\n",
        "]\n",
        "=\n",
        "ùúÜ\n",
        "E[X]=Œª\n",
        "\n",
        "-Variance:\n",
        "Var\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùúÜ\n",
        "Var(X)=Œª\n",
        "\n",
        "5. **Memoryless Property**: The Poisson process is memoryless; the probability of an event occurring in the next interval is independent of the past.\n",
        "\n",
        "### When to Use the Poisson Distribution\n",
        "\n",
        "- **Rare Events**: Appropriate for modeling the occurrence of rare events over a specified period or area (e.g., number of phone calls received at a call center in an hour).\n",
        "\n",
        "- **Fixed Interval**: Used when events occur independently and at a constant average rate within a fixed interval of time or space.\n",
        "\n",
        "- **Discrete Counts**: Suitable for situations where you count occurrences (e.g., defects in a batch of products, arrivals at a service point).\n",
        "\n",
        "### Summary\n",
        "\n",
        "The Poisson distribution is ideal for modeling the number of events in fixed intervals, especially when events are rare and occur independently at a constant rate."
      ],
      "metadata": {
        "id": "5vf5E5VrPurq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6 :  Define the terms \"probability distribution\" and \"probability density function\" (PDF). How does a PDF differ from a probability mass function (PMF)?\n",
        "\n",
        "\n",
        "### Probability Distribution\n",
        "\n",
        "A **probability distribution** describes how probabilities are assigned to the possible values of a random variable. It provides a complete description of the likelihood of different outcomes in an experiment or process. Probability distributions can be either discrete (for countable outcomes) or continuous (for uncountable outcomes).\n",
        "\n",
        "### Probability Density Function (PDF)\n",
        "\n",
        "A **probability density function (PDF)** is a function that describes the likelihood of a continuous random variable taking on a specific value. The PDF itself does not give probabilities directly; instead, the probability of the variable falling within a certain range is found by integrating the PDF over that range.\n",
        "\n",
        "### Difference Between PDF and PMF\n",
        "\n",
        "- **Type of Variable**:\n",
        "  - **PDF**: Used for continuous random variables (e.g., heights, weights).\n",
        "  - **PMF**: Used for discrete random variables (e.g., the number of heads in coin flips).\n",
        "\n",
        "- **Probability Representation**:\n",
        "  - **PDF**: The area under the curve of the PDF over an interval gives the probability of the variable falling within that interval. The total area under the PDF curve equals 1.\n",
        "  - **PMF**: The PMF gives the exact probability of a discrete outcome. The sum of all probabilities in a PMF equals 1.\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Probability Distribution**: General term for the likelihood of different outcomes.\n",
        "- **PDF**: Function for continuous variables; integrates to give probabilities over intervals.\n",
        "- **PMF**: Function for discrete variables; provides exact probabilities for specific outcomes."
      ],
      "metadata": {
        "id": "cPakv1cZQRyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7 :  Explain the Central Limit Theorem (CLT) with example.\n",
        "\n",
        "\n",
        "The **Central Limit Theorem (CLT)** is a fundamental statistical principle that states that, given a sufficiently large sample size, the distribution of the sample mean will approach a normal distribution, regardless of the shape of the population distribution, as long as the samples are independent and identically distributed (i.i.d).\n",
        "\n",
        "### Key Points of the CLT\n",
        "\n",
        "1. **Sample Size**: The larger the sample size (typically \\( n \\geq 30 \\) is considered sufficient), the more the sample mean will approximate a normal distribution.\n",
        "2. **Independence**: The samples must be independent of each other.\n",
        "3. **Population Distribution**: The original population can have any shape (normal, skewed, etc.).\n",
        "\n",
        "### Example of the Central Limit Theorem\n",
        "\n",
        "**Scenario**: Imagine you have a population of students in a school, and their exam scores are uniformly distributed between 0 and 100.\n",
        "\n",
        "1. **Population Distribution**: The distribution of scores is uniform, meaning that each score from 0 to 100 is equally likely.\n",
        "\n",
        "2. **Sampling**: You take multiple random samples of size \\( n = 30 \\) from this population and calculate the mean of each sample.\n",
        "\n",
        "3. **Distribution of Sample Means**: According to the CLT, if you plot the means of these samples, the distribution of those means will tend to be normally distributed, even though the original population of exam scores is not normally distributed.\n",
        "\n",
        "4. **Convergence to Normality**: As you increase the number of samples and keep sampling from the population, the histogram of the sample means will show a bell-shaped curve that resembles a normal distribution.\n",
        "\n",
        "### Importance of the CLT\n",
        "\n",
        "The CLT is significant because it allows statisticians to make inferences about population parameters using sample data. It justifies the use of normal distribution methods (like confidence intervals and hypothesis testing) for sample means, even when the population distribution is not normal, as long as the sample size is large enough."
      ],
      "metadata": {
        "id": "AKG15ghNQdMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8 :  Compare z-scores and t-scores. When should you use a z-score, and when should a t-score be applied instead?\n",
        "\n",
        "\n",
        "\n",
        "### Z-scores vs. T-scores\n",
        "\n",
        "**Z-scores** and **t-scores** are both standardized scores that indicate how many standard deviations a data point is from the mean, but they are used in different contexts and have distinct characteristics.\n",
        "\n",
        "### Z-scores\n",
        "\n",
        "- **Definition**: A z-score is used when the population standard deviation (\\( \\sigma \\)) is known.\n",
        "- **Distribution**: It follows the standard normal distribution (mean of 0 and standard deviation of 1).\n",
        "- **Usage**: Appropriate for large sample sizes (typically \\( n \\geq 30 \\)) or when the population is normally distributed.\n",
        "\n",
        "### T-scores\n",
        "\n",
        "- **Definition**: A t-score is used when the population standard deviation is unknown and must be estimated from the sample.\n",
        "- **Distribution**: It follows the t-distribution, which is similar to the normal distribution but has heavier tails, especially with smaller sample sizes.\n",
        "- **Usage**: Appropriate for small sample sizes (\\( n < 30 \\)) and when the population standard deviation is not known.\n",
        "\n",
        "### When to Use Each\n",
        "\n",
        "- **Use a Z-score**:\n",
        "  - When the population standard deviation is known.\n",
        "  - For large sample sizes (\\( n \\geq 30 \\)).\n",
        "  - When the data is normally distributed.\n",
        "\n",
        "- **Use a T-score**:\n",
        "  - When the population standard deviation is unknown.\n",
        "  - For small sample sizes (\\( n < 30 \\)).\n",
        "  - When the data may not be normally distributed but can be assumed to be approximately normal based on the Central Limit Theorem with a larger sample.\n",
        "\n",
        "### Summary\n",
        "\n",
        "In summary, use z-scores for known population standard deviations and larger samples, and use t-scores for unknown population standard deviations and smaller samples.\n"
      ],
      "metadata": {
        "id": "foLuF21OQrbi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9 :  Given a sample mean of 105, a population mean of 100, a standard deviation of 15, and a sample size of 25, calculate the z-score and p-value. Based on a significance level of 0.05, do you reject or fail to reject the null hypothesis? Task: Write Python code to calculate the z-score and p-value for the given data. Objective: Apply the formula for the z-score and interpret the p-value for hypothesis testing.\n",
        "\n",
        "\n",
        "To calculate the z-score and p-value given the sample mean, population mean, standard deviation, and sample size, you can use the following formulas:\n",
        "\n",
        "### Z-score Formula\n",
        "z= X\n",
        " ‚àíŒº/\n",
        "\n",
        "œÉ /square root(\n",
        "n)\n",
        "‚Äã\n",
        "\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "X\n",
        "  = sample mean (105)\n",
        "\n",
        "Œº = population mean (100)\n",
        "\n",
        "œÉ = population standard deviation (15)\n",
        "\n",
        "n = sample size (25)\n",
        "\n",
        "### P-value Calculation\n",
        "For a z-score, the p-value can be found using the cumulative distribution function (CDF) of the standard normal distribution.\n",
        "\n",
        "### Python Code\n",
        "Here‚Äôs how you can calculate the z-score and p-value using Python:\n",
        "\n",
        "```python\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Given values\n",
        "sample_mean = 105\n",
        "population_mean = 100\n",
        "std_dev = 15\n",
        "sample_size = 25\n",
        "\n",
        "# Calculate z-score\n",
        "z_score = (sample_mean - population_mean) / (std_dev / np.sqrt(sample_size))\n",
        "\n",
        "# Calculate p-value (two-tailed)\n",
        "p_value = 2 * (1 - stats.norm.cdf(z_score))\n",
        "\n",
        "# Output results\n",
        "print(f\"Z-score: {z_score:.2f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "# Decision based on significance level\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis.\")\n",
        "```\n",
        "\n",
        "### Interpretation\n",
        "1. **Calculate the Z-score**: This gives you how many standard deviations your sample mean is from the population mean.\n",
        "2. **Calculate the P-value**: This indicates the probability of observing a sample mean as extreme as 105 if the null hypothesis is true.\n",
        "3. **Decision**: If the p-value is less than the significance level (0.05), you reject the null hypothesis; otherwise, you fail to reject it.\n",
        "\n",
        "Running the code will provide the z-score and p-value, helping you make the hypothesis testing decision.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LxnhSC2zQ6pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10 :  Simulate a binomial distribution with 10 trials and a probability of success of 0.6 using Python. Generate 1,000 samples and plot the distribution. What is the expected mean and variance? Task: Use Python to generate the data, plot the distribution, and calculate the mean and variance.Objective: Understand the properties of a binomial distribution and verify them through simulation.\n",
        "\n",
        "To simulate a binomial distribution with 10 trials and a probability of success of 0.6, we can use Python's `numpy` library to generate samples and `matplotlib` to plot the distribution. Here‚Äôs how to do it:\n",
        "\n",
        "### Step-by-Step Code\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Parameters\n",
        "n_trials = 10          # Number of trials\n",
        "p_success = 0.6       # Probability of success\n",
        "n_samples = 1000      # Number of samples\n",
        "\n",
        "# Simulate binomial distribution\n",
        "samples = np.random.binomial(n_trials, p_success, n_samples)\n",
        "\n",
        "# Calculate mean and variance\n",
        "mean = np.mean(samples)\n",
        "variance = np.var(samples)\n",
        "\n",
        "# Print the expected mean and variance\n",
        "expected_mean = n_trials * p_success\n",
        "expected_variance = n_trials * p_success * (1 - p_success)\n",
        "\n",
        "print(f\"Simulated Mean: {mean:.2f}\")\n",
        "print(f\"Simulated Variance: {variance:.2f}\")\n",
        "print(f\"Expected Mean: {expected_mean:.2f}\")\n",
        "print(f\"Expected Variance: {expected_variance:.2f}\")\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(samples, bins=np.arange(-0.5, n_trials + 1.5, 1), kde=False, stat=\"density\")\n",
        "plt.title('Histogram of Binomial Distribution (n=10, p=0.6)')\n",
        "plt.xlabel('Number of Successes')\n",
        "plt.ylabel('Density')\n",
        "plt.xticks(range(n_trials + 1))\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Parameters**: We define the number of trials \\( n \\) as 10 and the probability of success \\( p \\) as 0.6.\n",
        "2. **Simulating Samples**: We generate 1,000 samples from the binomial distribution using `np.random.binomial`.\n",
        "3. **Calculating Mean and Variance**: We calculate both the simulated mean and variance from the samples and compare them with the expected values.\n",
        "   - **Expected Mean**: \\( E[X] = n \\times p = 10 \\times 0.6 = 6 \\)\n",
        "   - **Expected Variance**: \\( Var[X] = n \\times p \\times (1 - p) = 10 \\times 0.6 \\times 0.4 = 2.4 \\)\n",
        "4. **Plotting**: We plot the histogram of the sampled data using `seaborn` for better visualization.\n",
        "\n",
        "### Result Interpretation\n",
        "\n",
        "Running this code will display the histogram of the binomial distribution, showing the frequency of successes in the samples. The printed mean and variance will help verify the properties of the binomial distribution against the expected values. You should find that the simulated mean is close to 6 and the simulated variance is close to 2.4, demonstrating the properties of the binomial distribution effectively.\n"
      ],
      "metadata": {
        "id": "iCkQz7b8RzBs"
      }
    }
  ]
}